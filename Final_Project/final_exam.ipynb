{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753dc424",
   "metadata": {},
   "source": [
    "# Impact of Gender on Age Classification of Voice Recordings\n",
    "### Final Exam for Machine Learning and Deep Learning [CDSCO2004U]\n",
    "#### Daniel Henke, Jakob Hren, Heinrich Hegenbarth\n",
    "\n",
    "**Disclaimer:** This notebook is very long and computationally expensive. To be able to run core parts quickly, many outputs are hidden behind **global boolean variables** that one can set to True or False. Please set them according to needs and wishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CONVERSION=False\n",
    "\n",
    "EDA_AND_VISUALIZATION=False\n",
    "\n",
    "GRID_SEARCH=False\n",
    "\n",
    "MULTI_CLASSIFICATION=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa03415",
   "metadata": {},
   "source": [
    "## Imports & Installations\n",
    "\n",
    "We begin with possibly required installs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8250b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install librosa\n",
    "# !pip install playsound\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501c239",
   "metadata": {},
   "source": [
    "Here are all the imports needed for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0daf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from playsound import playsound\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer   \n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, log_loss, RocCurveDisplay\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197b95b",
   "metadata": {},
   "source": [
    "## Data Conversion\n",
    "\n",
    "**DISCLAIMER: This section is skippable, the exported data is available below.**\n",
    "\n",
    "To begin, we need to transform our data. We downloaded raw mp3 files in a tar.gz format from [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets). Our data includes the entire March 19 corpus of languages English, Spanish, French, German, Danish & Swedish. To start, we need to unpack all the .gz files. Ideally, one fully unpacks also the .tar file, however we have implemented a .tar extraction function as we only need very few audio files from the entire archive.\n",
    "\n",
    "### Overview & Filtering of Voice Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_overview(list_of_files, folder_path=None):\n",
    "    \"\"\"\n",
    "    This function takes a list of files and creates a joint dataframe for audio file conversion\n",
    "    \n",
    "    list_of_files: list of files to be converted\n",
    "    folder_path: path to the folder where the files are located\n",
    "    \"\"\"\n",
    "    overview = None\n",
    "    for file in list_of_files:\n",
    "        file_path = f\"{folder_path}/{file}\" if folder_path else file\n",
    "        with open(file_path, \"r\") as file:\n",
    "            if overview is None:\n",
    "                overview = pd.read_csv(file, sep=\"\\t\", dtype={'sentence_domain': str}, low_memory=False)\n",
    "            else:\n",
    "                overview = pd.concat([overview, pd.read_csv(file, sep=\"\\t\", dtype={'sentence_domain': str}, low_memory=False)], ignore_index=True)\n",
    "    return overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3354083",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_CONVERSION:\n",
    "    list_of_files = [\"validated.tsv\",\"other.tsv\"]\n",
    "    folder_path_danish=\"./data/cv-corpus-21.0-2025-03-14/da\"\n",
    "    folder_path_swedish=\"./data/cv-corpus-21.0-2025-03-14/sv-SE\"\n",
    "    folder_path_german=\"./data/cv-corpus-21.0-2025-03-14/de\"\n",
    "    folder_path_french=\"./data/cv-corpus-21.0-2025-03-14/fr\"\n",
    "    folder_path_spanish=\"./data/cv-corpus-21.0-2025-03-14/es\"\n",
    "    folder_path_english=\"./data/cv-corpus-21.0-2025-03-14/en\"\n",
    "    folder_path=folder_path_english\n",
    "    overview = prepare_overview(list_of_files,folder_path)\n",
    "    overview.info()\n",
    "    overview.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.groupby(\"gender\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3339c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_gender(gender):\n",
    "    if gender == \"male\":\n",
    "        return \"male_masculine\"\n",
    "    elif gender == \"female\":\n",
    "        return \"female_feminine\"\n",
    "    else:\n",
    "        return gender\n",
    "\n",
    "\n",
    "def preprocess_overview(overview, f_path):\n",
    "    \"\"\"\n",
    "    This function takes a dataframe and preprocesses it for audio file conversion\n",
    "    \"\"\"\n",
    "    overview = overview.dropna(subset=[\"gender\"])\n",
    "    # irrelevant columns for our analysis\n",
    "    overview = overview.drop(columns=[\"variant\", \"segment\", \"sentence_id\", \"up_votes\", \"down_votes\"])\n",
    "    # fixing gender labels\n",
    "    overview[\"gender\"] = overview[\"gender\"].apply(fix_gender)\n",
    "    # dropping all files that are not simply male or female\n",
    "    # this is not a political statement, simply we do not have enough data for other gender classifications\n",
    "    overview = overview[(overview[\"gender\"] == \"female_feminine\") | (overview[\"gender\"] == \"male_masculine\")]\n",
    "    # limiting to a maximum of 5 random clips per client_id\n",
    "    overview = overview.groupby(\"client_id\").apply(lambda group: group.sample(n=min(len(group), 5), random_state=42)).reset_index(drop=True)\n",
    "    # changing the path to reflect the location of the audio files\n",
    "    overview[\"path\"] = overview[\"path\"].apply(lambda x: f\"{f_path}/{x}\")\n",
    "    return overview.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d0d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On my device, all audio files are in the same \"clips\" folder. Please change the path if your files are in a different location.\n",
    "overview=preprocess_overview(overview, \"clips\")\n",
    "overview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd660a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd00c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.groupby(\"gender\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.groupby(\"age\").size()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
